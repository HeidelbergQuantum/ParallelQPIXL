{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCIENCE\n",
    "\n",
    "Of course, all is well and fun, and we can visualize some cool quantum operations on an 'inmage state', but obviously the main reason for these embeddings is not to make nice pictures, but to use them to encode data for tasks like classification for QML, or we can use image embedding to encode different types of data - linear depth is a pretty nice embedding ratio after-all! \n",
    "\n",
    "# QML with QPIXL embedding with classical autoencoder for image compression\n",
    "\n",
    "So instead of using just a directly image loaded QML, we first pretrain a classical autoencoder and do a transfer learning approach. FOr this toy example we use a pretrained resnet18, but one could imagine refining model parameters to a dataset at the same time as the quantum weights! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# Import the relevant packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import param_qpixl_qml as pq\n",
    "from qiskit.circuit import ParameterVector\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# SETUP\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "n_qubits = 11                # Number of qubits\n",
    "step = 0.0004               # Learning rate\n",
    "batch_size = 2            # Number of samples for each training step\n",
    "num_epochs = 30              # Number of training epochs\n",
    "q_depth = 6                 # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01              # Initial spread of random quantum weights\n",
    "start_time = time.time()    # Start of the computation timer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 140\n",
    "\n",
    "X_train = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(),transforms.Lambda(lambda x: torch.flatten(x))]),\n",
    ")\n",
    "\n",
    "# Leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_train.targets == 0)[0][:sample_count],\n",
    "    np.where(X_train.targets == 1)[0][:sample_count],\n",
    ")\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True)\n",
    "\n",
    "# Test set\n",
    "sample_count = 70\n",
    "\n",
    "X_test = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(),transforms.Lambda(lambda x: torch.flatten(x))]),\n",
    ")\n",
    "idx = np.append(\n",
    "    np.where(X_test.targets == 0)[0][:sample_count],\n",
    "    np.where(X_test.targets == 1)[0][:sample_count],\n",
    ")\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "X_test.data.reshape((X_test.data.shape[0],X_test.data.shape[1]**2))\n",
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "    \n",
    "dataloaders = {\n",
    "    'train':train_loader,'test':test_loader\n",
    "}\n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in [\"train\", \"test\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNN \n",
    "We use a tree tensor network ansatz to try and do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## QUANTUM NN\n",
    "def block(weights, wires):\n",
    "    qml.RX(weights[0], wires=wires[0])\n",
    "    qml.RZ(weights[1], wires=wires[0])\n",
    "    qml.RX(weights[2], wires=wires[1])\n",
    "    qml.RZ(weights[3], wires=wires[1])\n",
    "    qml.CNOT(wires=wires)\n",
    "    \n",
    "dev = qml.device(\"lightning.gpu\", wires=n_qubits+5)\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(input_feat,weights):\n",
    "    quantum_input = pq.encode_image(input_feat)\n",
    "    pq.param_qpixl(quantum_input) ## PENNYL:ANE QPIXL\n",
    "    qml.TTN(wires=range(n_qubits+5),\n",
    "            n_block_wires=2,\n",
    "            block=block,\n",
    "            n_params_block=4,\n",
    "            template_weights=weights)\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in [14,15]]\n",
    "    return exp_vals\n",
    "def costfunc(params):\n",
    "    cost = 0\n",
    "    for i in range(len(BAS)):\n",
    "        if i < len(BAS) / 2:\n",
    "            cost += circuit(BAS[i], params)\n",
    "        else:\n",
    "            cost -= circuit(BAS[i], params)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid QNN with classical autoencoder\n",
    "We use a classical autoencoder to reduce the dimensions in such a way that the image highlights features we want to classify with the QNN, where the embedding is done with QPIXL and the trained weights affect the tensor network tree. Our loss is cross entropy, the preffered method for classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.pre_net = nn.Linear(28*28,1)\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn((15,4)))\n",
    "        self.post_net = nn.Linear(2, 2)\n",
    "        self.float()\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(input_features) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, 2)\n",
    "        q_out = q_out.to(device)\n",
    "        q_in = q_in.to(torch.float64)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = [quantum_net(elem, self.q_params)]\n",
    "            q_out_elem = torch.tensor(q_out_elem,requires_grad=True).to(device)\n",
    "            q_out = torch.cat((q_out,q_out_elem))\n",
    "            q_out = q_out.to(torch.float)\n",
    "            # q_out[0] = q_out_elem[0]\n",
    "            # q_out[1] = q_out_elem[1]\n",
    "\n",
    "            \n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for param in model_hybrid.parameters():\n",
    "#     param.requires_grad = False\n",
    "model_hybrid = DressedQuantumNet()\n",
    "model_hybrid = model_hybrid.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_hybrid = optim.Adam(model_hybrid.parameters(), lr=step)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "    optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler\n",
    ")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10000.0  # Large arbitrary number\n",
    "    best_acc_train = 0.0\n",
    "    best_loss_train = 10000.0  # Large arbitrary number\n",
    "    print(\"Training started:\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            if phase == \"train\":\n",
    "                # Set model to training mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to evaluate mode\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            it = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                since_batch = time.time()\n",
    "                batch_size_ = len(inputs)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track/compute gradient and make an optimization step only when training\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Print iteration results\n",
    "                running_loss += loss.item() * batch_size_\n",
    "                batch_corrects = torch.sum(preds == labels.data).item()\n",
    "                running_corrects += batch_corrects\n",
    "                print(\n",
    "                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n",
    "                        phase,\n",
    "                        epoch + 1,\n",
    "                        num_epochs,\n",
    "                        it + 1,\n",
    "                        n_batches + 1,\n",
    "                        time.time() - since_batch,\n",
    "                    ),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                it += 1\n",
    "\n",
    "            # Print epoch results\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(\n",
    "                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n",
    "                    \"train\" if phase == \"train\" else \"test  \",\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                    epoch_acc,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Check if this is the best model wrt previous epochs\n",
    "            if phase == \"test\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"test\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
    "                best_acc_train = epoch_acc\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "\n",
    "            # Update learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
    "    )\n",
    "    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pennylane/math/utils.py:227: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train Epoch: 1/30 Loss: 0.6953 Acc: 0.5000        \n",
      "Phase: test   Epoch: 1/30 Loss: 0.6947 Acc: 0.5000        \n",
      "Phase: train Epoch: 2/30 Loss: 0.6947 Acc: 0.5000        \n",
      "Phase: test   Epoch: 2/30 Loss: 0.6942 Acc: 0.5000        \n",
      "Phase: train Epoch: 3/30 Iter: 248/141 Batch time: 0.4315\r"
     ]
    }
   ],
   "source": [
    "model_hybrid = train_model(\n",
    "    model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "07b300163aa49736bc99c2e65c19119dd077db61d3deffc047b36e340452655c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
